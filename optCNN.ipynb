{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optCNN: AI Inference Optimization for Deep CNNs\n",
    "Welcome to the outcome presentation of Part 1 of the series. Here, we will conduct a performance analysis of four different AI inference optimization levels. These levels include:\n\n",
    "    1. Standard PyTorch Inference\n",
    "    2. ONNX Runtime Inference\n",
    "    3. TensorRT Inference\n",
    "    4. TensorRT Mixed-Precision Inference\n",
    "\n",
    "To ensure an apples-to-apples comparison, we will perform inference on the same GPU for all level. Our example model will be ResNet-50, a popular deep convolutional network. In this analysis, we will focus solely on the speed of inference rather than the quality of the results.\n",
    "\n",
    "We will use **<span style=\"color:green\">Nsight Systems</span>** to profile each approach and gain insights into their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings # This is merely a presentation notebook.\n",
    "warnings.filterwarnings(\"ignore\") # We'd like it clean.\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import pycuda.driver as cuda\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch GPU Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching model and Benchmarking Inference in PyTorch's native space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch-GPU Baseline Inference Time: 0.017316 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the PyTorch model\n",
    "model = models.resnet50(pretrained=True).eval().cuda()\n",
    "\n",
    "# Prepare input data\n",
    "input_image = torch.randn(1, 3, 224, 224).cuda()\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(10):\n",
    "    _ = model(input_image)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):\n",
    "        _ = model(input_image)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'PyTorch-GPU Baseline Inference Time: {(end_time - start_time) / 100:.6f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PyTorch-GPU Baseline Inference Time: 0.017316 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![High Level Nsight-Systems Timeline Screenshot here](reports/snaps/torch_out.png)\n",
    "\n",
    "The 17ms highlighted window corresponds to 1 inference (1 iteration of the loop above). We can use the patterns in the CUDA-HW memory channel as markers for iteration identification. We observe high variation between the durations of individual iterations, indicating room for optimization.\n",
    "\n",
    "![Single iteration Level Nsight-Systems Timeline Screenshot here](reports/snaps/torch_in.png)\n",
    "\n",
    "Zooming in, we observe frequent and significant gaps between kernel executions. This is evident in the CUDA-HW kernel channel, where we see an alternation between kernel execution and idle times. Over an entire inference, these add up to be significant, with almost 50% of the inference window consisting of kernel idle time.\n",
    "\n",
    "![A few kernel Level Nsight-Systems Timeline Screenshot here](reports/snaps/torch_in_in.png)\n",
    "\n",
    "Looking closer, we see that each kernel block (see streams channel) is preceded by a corresponding call to the kernel (CUDA API channel). The CUDA API makes a call to the kernel, followed by kernel execution, then idle time until another request is made. This indicates a need for better kernel request handling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ONNX Runtime Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading serialized version of the model and benchmarking on ONNX Runtime Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime Inference Time: 0.003505 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the ONNX model with GPU (CUDA) execution provider \n",
    "onnx_model_path = \"models/resnet50.onnx\"\n",
    "providers = ['CUDAExecutionProvider'] #if 'CUDAExecutionProvider' in ort.get_available_providers()# else ['CPUExecutionProvider']\n",
    "session = ort.InferenceSession(onnx_model_path, providers=providers)\n",
    "\n",
    "# Prepare input data\n",
    "input_image = np.random.randn(1, 3, 224, 224).astype(np.float32)\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(10):\n",
    "    _ = session.run(None, {\"input\": input_image})\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    _ = session.run(None, {\"input\": input_image})\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'ONNX Runtime Inference Time: {(end_time - start_time) / 100:.6f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ONNX Runtime Inference Time: 0.003505 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![High Level Nsight-Systems Timeline Screenshot here](reports/snaps/onnx_out.png)\n",
    "\n",
    "Using the patterns in the CUDA HW Memory channel as markers, we cut out a single 3.6 ms inference window. The iteration durations are much more consistent compared to PyTorch.\n",
    "\n",
    "![A single iteration Level Nsight-Systems Timeline Screenshot here](reports/snaps/onnx_in.png)\n",
    "\n",
    "A closer look reveals reduced idle times between CUDA API calls to kernels. The majority of the inference window is now occupied by kernel execution, with minimal idle time. Speedups should now come from accelerating individual kernel executions.\n",
    "\n",
    "![A few kernel Level Nsight-Systems Timeline Screenshot here](reports/snaps/onnx_in_in.png)\n",
    "\n",
    "Going deeper, we still see an alternation between kernel calls and executions, but the API calls are more evenly distributed, resulting in lower idle times. Queuing kernel call requests could further resolve this issue. We also observe more balanced activity across multiple CUDA streams, indicating a better-distributed workload and improved parallelism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TensorRT Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading ONNX-serialized model and Benchmarking TensorRT-accelerated Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CUDA\n",
    "cuda.init()\n",
    "\n",
    "# Create CUDA context\n",
    "device = cuda.Device(0)\n",
    "cuda_context = device.make_context()\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = \"models/resnet50.onnx\"\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Create a TensorRT logger\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the TensorRT engine\n",
    "def build_engine(onnx_file_path):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        \n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # 1GB\n",
    "\n",
    "        with open(onnx_file_path, 'rb') as model:\n",
    "            if not parser.parse(model.read()):\n",
    "                print('Failed to parse the ONNX file')\n",
    "                for error in range(parser.num_errors):\n",
    "                    print(parser.get_error(error))\n",
    "                return None\n",
    "            \n",
    "        serialized_engine = builder.build_serialized_network(network, config)\n",
    "        if serialized_engine is None:\n",
    "            print(\"Failed to build the engine.\")\n",
    "            return None\n",
    "        \n",
    "        runtime = trt.Runtime(TRT_LOGGER)\n",
    "        return runtime.deserialize_cuda_engine(serialized_engine)\n",
    "\n",
    "engine = build_engine(onnx_model_path)\n",
    "if engine is None:\n",
    "    print(\"Engine could not be created.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate buffers\n",
    "inputs, outputs, bindings, stream = [], [], [], cuda.Stream()\n",
    "for i in range(engine.num_bindings):\n",
    "    binding = engine.get_tensor_name(i)\n",
    "    size = trt.volume(engine.get_tensor_shape(binding))\n",
    "    dtype = trt.nptype(engine.get_tensor_dtype(binding))\n",
    "    host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "    device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "    bindings.append(int(device_mem))\n",
    "    if engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT:\n",
    "        inputs.append((host_mem, device_mem))\n",
    "    else:\n",
    "        outputs.append((host_mem, device_mem))\n",
    "\n",
    "# Create context\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data\n",
    "input_image = cv2.imread(\"data/croc.jpeg\")\n",
    "input_image = cv2.resize(input_image, (224, 224))\n",
    "input_image = input_image.astype(np.float32)\n",
    "input_image = input_image.transpose(2, 0, 1)  # HWC to CHW\n",
    "input_image = np.expand_dims(input_image, axis=0)  # Add batch dimension\n",
    "input_image = np.ascontiguousarray(input_image)\n",
    "\n",
    "# Copy input data to pagelocked buffer\n",
    "np.copyto(inputs[0][0], input_image.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT Inference Time: 0.001817 seconds\n"
     ]
    }
   ],
   "source": [
    "# Warm-up\n",
    "for _ in range(10):\n",
    "    cuda.memcpy_htod_async(inputs[0][1], inputs[0][0], stream)\n",
    "    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "    cuda.memcpy_dtoh_async(outputs[0][0], outputs[0][1], stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    cuda.memcpy_htod_async(inputs[0][1], inputs[0][0], stream)\n",
    "    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "    cuda.memcpy_dtoh_async(outputs[0][0], outputs[0][1], stream)\n",
    "    stream.synchronize()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'TensorRT Inference Time: {(end_time - start_time) / 100:.6f} seconds')\n",
    "\n",
    "# Clean up\n",
    "del context\n",
    "del engine\n",
    "cuda_context.pop()\n",
    "del cuda_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TensorRT Inference Time: 0.001817 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![High Level Nsight-Systems Timeline Screenshot here](reports/snaps/tesnorrt_out.png)\n",
    "\n",
    "We now see a dedicated TensorRT channel. Using this and the CUDA HW memory patterns, we isolate an inference.\n",
    "\n",
    "![Single iteration Level Nsight-Systems Timeline Screenshot here](reports/snaps/tensorrt_in.png)\n",
    "\n",
    "We observe that CUDA API calls for kernels are queued together, enabling a contiguous kernel execution block with virtually no idle time once execution begins. The number of kernels has reduced, and they are different from previous methods due to kernel auto-tuning and layer fusion, which significantly reduce total kernel execution time. However, the large cuStreamSynchronize block suggests potential for further optimization by speeding up individual kernels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantized TensorRT Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking mixed-precision quantized model using TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CUDA\n",
    "cuda.init()\n",
    "\n",
    "# Create CUDA context\n",
    "device = cuda.Device(0)\n",
    "cuda_context = device.make_context()\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = \"models/resnet50.onnx\"\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Create a TensorRT logger\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define INT8 Calibrator class\n",
    "class PythonEntropyCalibrator(trt.IInt8EntropyCalibrator2):\n",
    "    def __init__(self, data, batch_size):\n",
    "        trt.IInt8EntropyCalibrator2.__init__(self)\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.current_index = 0\n",
    "        self.device_input = cuda.mem_alloc(data.nbytes)\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def get_batch(self, names):\n",
    "        if self.current_index + self.batch_size > self.data.shape[0]:\n",
    "            return None\n",
    "        batch = self.data[self.current_index:self.current_index + self.batch_size].ravel()\n",
    "        cuda.memcpy_htod(self.device_input, batch)\n",
    "        self.current_index += self.batch_size\n",
    "        return [int(self.device_input)]\n",
    "\n",
    "    def read_calibration_cache(self):\n",
    "        return None\n",
    "\n",
    "    def write_calibration_cache(self, cache):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the engine\n",
    "def build_engine(onnx_file_path):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        \n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # 1GB\n",
    "        \n",
    "        if builder.platform_has_fast_fp16:\n",
    "            config.set_flag(trt.BuilderFlag.FP16)\n",
    "        if builder.platform_has_fast_int8:\n",
    "            config.set_flag(trt.BuilderFlag.INT8)\n",
    "            # Create dummy calibration data for example purposes\n",
    "            dummy_data = np.random.random((100, 3, 224, 224)).astype(np.float32)\n",
    "            calibrator = PythonEntropyCalibrator(dummy_data, batch_size=1)\n",
    "            config.int8_calibrator = calibrator\n",
    "\n",
    "        with open(onnx_file_path, 'rb') as model:\n",
    "            if not parser.parse(model.read()):\n",
    "                print('Failed to parse the ONNX file')\n",
    "                for error in range(parser.num_errors):\n",
    "                    print(parser.get_error(error))\n",
    "                return None\n",
    "            \n",
    "        serialized_engine = builder.build_serialized_network(network, config)\n",
    "        if serialized_engine is None:\n",
    "            print(\"Failed to build the engine.\")\n",
    "            return None\n",
    "        \n",
    "        runtime = trt.Runtime(TRT_LOGGER)\n",
    "        return runtime.deserialize_cuda_engine(serialized_engine)\n",
    "\n",
    "engine = build_engine(onnx_model_path)\n",
    "if engine is None:\n",
    "    print(\"Engine could not be created.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate buffers\n",
    "inputs, outputs, bindings, stream = [], [], [], cuda.Stream()\n",
    "for i in range(engine.num_bindings):\n",
    "    binding = engine.get_tensor_name(i)\n",
    "    size = trt.volume(engine.get_tensor_shape(binding))\n",
    "    dtype = trt.nptype(engine.get_tensor_dtype(binding))\n",
    "    host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "    device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "    bindings.append(int(device_mem))\n",
    "    if engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT:\n",
    "        inputs.append((host_mem, device_mem))\n",
    "    else:\n",
    "        outputs.append((host_mem, device_mem))\n",
    "\n",
    "# Create context\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data\n",
    "input_image = cv2.imread(\"data/croc.jpeg\")\n",
    "input_image = cv2.resize(input_image, (224, 224))\n",
    "input_image = input_image.astype(np.float32)\n",
    "input_image = input_image.transpose(2, 0, 1)  # HWC to CHW\n",
    "input_image = np.expand_dims(input_image, axis=0)  # Add batch dimension\n",
    "input_image = np.ascontiguousarray(input_image)\n",
    "\n",
    "# Copy input data to pagelocked buffer\n",
    "np.copyto(inputs[0][0], input_image.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized TensorRT Inference Time: 0.000506 seconds\n"
     ]
    }
   ],
   "source": [
    "# Warm-up\n",
    "for _ in range(10):\n",
    "    cuda.memcpy_htod_async(inputs[0][1], inputs[0][0], stream)\n",
    "    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "    cuda.memcpy_dtoh_async(outputs[0][0], outputs[0][1], stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    cuda.memcpy_htod_async(inputs[0][1], inputs[0][0], stream)\n",
    "    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "    cuda.memcpy_dtoh_async(outputs[0][0], outputs[0][1], stream)\n",
    "    stream.synchronize()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Quantized TensorRT Inference Time: {(end_time - start_time) / 100:.6f} seconds')\n",
    "\n",
    "# Clean up\n",
    "del context\n",
    "del engine\n",
    "cuda_context.pop()\n",
    "del cuda_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantized TensorRT Inference Time: 0.000506 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Single iteration Level Nsight-Systems Timeline Screenshot here](reports/snaps/quant_in.png)\n",
    "\n",
    "We now see negligible cuStreamSynchronize time, indicating that individual kernels execute very quickly due to INT8 calibration and FP16 mixed precision. The execution time is so fast that we start to see kernel idle time again, waiting for the next kernel call to be queued. The next steps would be to test the impact of lower precision on result quality, improve the management of the Memcpy HtoD block, and speed up the queuing of kernel calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Inference Type            | Time (seconds) |\n",
    "|---------------------------|----------------|\n",
    "| PyTorch GPU               | 0.017001       |\n",
    "| ONNX-Runtime GPU          | 0.003505       |\n",
    "| TensorRT                  | 0.001817       |\n",
    "| TensorRT + Quantization   | 0.000506       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inference time reduced to 2% the baseline GPU time'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually inserted values for now\n",
    "f'Inference time reduced to {int((0.000506/0.017001) * 100)}% the baseline GPU time'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While PyTorch with CUDA helps parallelize our model with GPUs, it has limitations in its native form. Moving to ONNX leverages operator-level optimizations and more evenly distributed CUDA API calls, significantly reducing GPU idle times. Transitioning to TensorRT provides scenario-tailored optimizations such as kernel auto-tuning, layer fusion, advanced memory management, and better kernel scheduling through meaningful queuing of CUDA API calls. Finally, reducing numerical precision enhances execution and memory transfer efficiency, resulting in lower latency and higher throughput."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
